**Summary:**  
I have 2 years of experience in python development having started back in 2018. I am involved into programming since university, where I gained knowledge in databases, OOP, C#, R, JS, algorithms, structures and etc. So that helped me in getting my first job experience.
I am organized and result-driven person.  I try to keep my code clean and easy to read so if I have to go back and make some changes, I don’t get mixed up and confused. I am responsible and communicative person by my own, and it helps me to be a good team member. I always try to take part in important decisions and searching for compromise in arguable situations. I am always interested in improving my skills in development.  

**QUALIFICATION:**  
    • Fluency in Python  
    • Front-end:  CSS , HTML, Bootstrap  
    • Back-end: Django, Falcon  
    • Databases:  
    - RDBMS: Postgresql, Clickhouse  
    - NoSQL: Redis, Arango  
    - ORMs: Sqlalchemy, Peewee  
    • Git: GitHub, BitBucket  
    • Docker  
    • Apache Kafka  
    
**OPTIONAL BONUS SKILLS:**  
    • Basic knowledge of C#, R, Ruby, JS  
    • Basic knowledge of React, Node JS, Ruby on Rails  
    • Some experience with CI/CD  
Understanding of OOP, SOLID principles, Agile methodologies, TDD, design patterns.  

**LAST PROJECTS:**  
• URL-Shortener  
Stack: Python, Falcon, Postgresql, Docker  
I was responsible for back-end. I developed services using Falcon REST API framework. In this project, I implemented a search service, CRUD, authentication, etc. I also wrote and optimized SQL queries.    

• Micro-services for preparing data sets  
Stack: Python, Kafka, Clickhouse, Docker, Graphite, Grafana  
I developed a service that took events from a kafka and then added the necessary fields and deleted unnecessary ones. Then the app sent events to the clickhouse. Also, this application sent data on the number of processed events per unit of time to Graphite for their further display in Grafana.  

• Script for import Facebook leads  
Stack: Python, Kafka, request, Docker, Postgresql  
I developed a script to import data from Facebook. This script was runing on schedule. Script took the data about import from the database, then based on this data it took necessary data from Facebook API. This script turned the data into the required form and sent it to the kafka. This script ended up  updating the data about import in the database.  

**Education**  
-Polotsk State University 2020-2020 (Bachelor's degree, Information Technology Software)  
-Minsk Innovation University 2016-2020 (Bachelor's degree, Information Technology Software)  

**Professional experience:**  
Epica,Data enineer, August 2018 - August 2020  
Development and support of microservices for collecting and processing user information. 
Support and development of applications for integration with various platforms  


**Contacts:**  
Dima Shybut  
[linkedin](https://www.linkedin.com/in/dmitriy-shybut/)  
shybut69@gmail.com  
